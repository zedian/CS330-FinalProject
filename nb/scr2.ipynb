{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aba76c0-1564-4de7-827f-f2738c24f622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gautam/Desktop/workbench/cs330-final\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de431770-d30a-441a-b42a-19cb505b4eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28936bd5-4bfe-4ef2-b83f-41f64d29c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from model import Shareable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a6745e71-9d68-450a-924f-5c94479dade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512\n",
    "lr = 3e-4\n",
    "\n",
    "is_two_train = datasets.IsNumber(n=2, train=True)\n",
    "is_five_train = datasets.IsNumber(n=5, train=True)\n",
    "\n",
    "is_two_val = datasets.IsNumber(n=2, train=False)\n",
    "is_five_val = datasets.IsNumber(n=5, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9b957296-b20e-4756-b40c-7cac4904acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_two_train_loader = DataLoader(is_two_train, batch_size=bs, shuffle=True, drop_last=True)\n",
    "is_five_train_loader = DataLoader(is_five_train, batch_size=bs, shuffle=True, drop_last=True)\n",
    "\n",
    "is2_iter = itertools.cycle(is_two_train_loader)\n",
    "is5_iter = itertools.cycle(is_five_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bdf46070-125a-4342-bc0e-4e6f49c0d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_dict(loss, model):\n",
    "    names, params = zip(*model.named_parameters())\n",
    "    grads = torch.autograd.grad(loss, params, allow_unused=True, retain_graph=True)\n",
    "    zipped_grads = list(zip(names, grads))\n",
    "    return dict(zipped_grads)\n",
    "\n",
    "\n",
    "def clone_grads(model):\n",
    "    names, params = zip(*model.named_parameters())\n",
    "    grads = [p.grad if p.grad is None else p.grad.clone() for p in params]\n",
    "    return dict(zip(names, grads))\n",
    "\n",
    "\n",
    "def sub_state_dicts(a, b):\n",
    "    assert a.keys() == b.keys()\n",
    "    a_vals = a.values()\n",
    "    b_vals = b.values()\n",
    "    return {k: (v1 if v1 is not None else 0) - (v2 if v2 is not None else 0) \n",
    "            for k, v1, v2 in zip(a.keys(), a_vals, b_vals)}\n",
    "\n",
    "\n",
    "def stack_grad(grad_dict_list, task_name, param_name, flatten=True):\n",
    "    grads = [g[task_name][param_name] for g in grad_dict_list]\n",
    "    return torch.stack(grads).view(len(grads), -1)\n",
    "\n",
    "\n",
    "def low_pass_filter(x, filter_size=25):\n",
    "    x = torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n",
    "    x_smooth = F.conv1d(x[None], torch.ones(1, 1, filter_size) / filter_size)\n",
    "    return x_smooth.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ed293f8b-473f-42da-931f-a10ee7c2a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x.shape == (B, 1, 28, 28)\n",
    "        return self.net(x).view(x.shape[0], -1)\n",
    "        \n",
    "        \n",
    "class LinearBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x.shape == (B, 1, 28, 28)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.net(x).view(x.shape[0], -1)\n",
    "    \n",
    "\n",
    "class SharedMTL(nn.Module):\n",
    "    def __init__(self, task_keys):\n",
    "        super().__init__()\n",
    "        self.backbone = ConvBackbone()\n",
    "        self.heads = nn.ModuleDict({\n",
    "            task: nn.Linear(256, 1)\n",
    "            for task in task_keys\n",
    "        })\n",
    "    \n",
    "    def forward(self, x, task):\n",
    "        return self.heads[task](self.backbone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "95e63b15-9e5a-49a7-9d05-96484d592baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 {'t0': 0.7179786562919617, 't1': 0.6881766319274902}\n",
      "step 10 {'t0': 0.31588083505630493, 't1': 0.31994783878326416}\n",
      "step 20 {'t0': 0.28396108746528625, 't1': 0.43509432673454285}\n",
      "step 30 {'t0': 0.3228101432323456, 't1': 0.2658112347126007}\n",
      "step 40 {'t0': 0.3149857223033905, 't1': 0.2961196005344391}\n",
      "step 50 {'t0': 0.2839863896369934, 't1': 0.3070862293243408}\n",
      "step 60 {'t0': 0.298997700214386, 't1': 0.29681938886642456}\n",
      "step 70 {'t0': 0.27741920948028564, 't1': 0.28228747844696045}\n",
      "step 80 {'t0': 0.2513234615325928, 't1': 0.2829279899597168}\n",
      "step 90 {'t0': 0.22837910056114197, 't1': 0.25727617740631104}\n",
      "step 100 {'t0': 0.23665127158164978, 't1': 0.25955528020858765}\n",
      "step 110 {'t0': 0.2073015570640564, 't1': 0.21563036739826202}\n",
      "step 120 {'t0': 0.16826166212558746, 't1': 0.26743876934051514}\n",
      "step 130 {'t0': 0.1875883936882019, 't1': 0.24544239044189453}\n",
      "step 140 {'t0': 0.12857620418071747, 't1': 0.23376533389091492}\n",
      "step 150 {'t0': 0.11570951342582703, 't1': 0.1712069809436798}\n",
      "step 160 {'t0': 0.12790820002555847, 't1': 0.19542533159255981}\n",
      "step 170 {'t0': 0.08467917144298553, 't1': 0.18988457322120667}\n",
      "step 180 {'t0': 0.0720670223236084, 't1': 0.16767889261245728}\n",
      "step 190 {'t0': 0.1518026739358902, 't1': 0.17539730668067932}\n"
     ]
    }
   ],
   "source": [
    "steps = 200\n",
    "\n",
    "tasks = {\n",
    "    't0': {\n",
    "        'data': is2_iter,\n",
    "        'loss': lambda yh, y: F.binary_cross_entropy_with_logits(yh, y[..., None]),\n",
    "    },\n",
    "    't1': {\n",
    "        'data': is5_iter,\n",
    "        'loss': lambda yh, y: F.binary_cross_entropy_with_logits(yh, y[..., None]),\n",
    "    },\n",
    "}\n",
    "mtl = SharedMTL(tasks.keys())\n",
    "opt = torch.optim.Adam(mtl.parameters(), lr=lr)\n",
    "\n",
    "step = 0\n",
    "losses = []\n",
    "grads = []\n",
    "while step < steps:\n",
    "    task_losses = {}\n",
    "    task_grads = {}\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    for task_name, task_iter in tasks.items():\n",
    "        batch = next(task_iter)\n",
    "        x, y = batch\n",
    "        pred = mtl(x[:, None, ...], task_name)\n",
    "        loss = tasks[task_name][\n",
    "        \n",
    "        F.binary_cross_entropy_with_logits(pred, y[..., None])\n",
    "        \n",
    "        # avoid gradient accumulation bugs\n",
    "        running_grads = clone_grads(mtl)\n",
    "        loss.backward()\n",
    "        task_grads[task_name] = sub_state_dicts(clone_grads(mtl), running_grads)\n",
    "        task_losses[task_name] = loss.item()\n",
    "    \n",
    "    opt.step()\n",
    "\n",
    "    losses.append(task_losses)\n",
    "    grads.append(task_grads)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('step', step, task_losses)\n",
    "    \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "63454b2b-027c-47a8-bf12-ff81d92fb3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: \t\t\tdir_s\tvar\n",
      "backbone.net.0.weight: \t0.4217\t0.1997\n",
      "backbone.net.0.bias: \t0.2994\t0.3110\n",
      "backbone.net.3.weight: \t0.1899\t0.1822\n",
      "backbone.net.3.bias: \t0.1528\t0.2950\n",
      "backbone.net.6.weight: \t0.0307\t0.0240\n",
      "backbone.net.6.bias: \t0.0387\t0.0388\n"
     ]
    }
   ],
   "source": [
    "param_keys = ['backbone.' + k for k in list(mtl.backbone.state_dict().keys())]\n",
    "\n",
    "print('param: \\t\\t\\tdir_s\\tvar')\n",
    "\n",
    "values = []\n",
    "\n",
    "for key in param_keys:\n",
    "    g0 = stack_grad(grads, 't0', key)\n",
    "    g1 = stack_grad(grads, 't1', key)\n",
    "    \n",
    "    cosine = torch.sum(F.normalize(g0, dim=-1) * F.normalize(g1, dim=-1), dim=-1)\n",
    "    smooth_cos = low_pass_filter(cosine, filter_size=10)[0]\n",
    "    \n",
    "    print(f'{key}: \\t{smooth_cos.mean():.4f}\\t{cosine.var():.4f}')\n",
    "    values.append(smooth_cos.mean())\n",
    "    \n",
    "    # plt.title(key)\n",
    "    # plt.ylim([-1.1, 1.1])\n",
    "    # plt.plot(cosine)\n",
    "    # plt.plot(smooth_cos)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cfa74ae3-5165-40bd-962a-a33aa1bf0dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37213403, 0.26421025, 0.16755864, 0.13485323, 0.03414306,\n",
       "       0.02710086], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = sorted(values, reverse=True)\n",
    "v / np.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9d156-096a-45d9-829e-9113a1b418df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dde99-1313-450e-9b33-30b8c4605cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
