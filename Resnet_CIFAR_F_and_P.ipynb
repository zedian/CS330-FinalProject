{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2cbc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdx_macos/.pyenv/versions/meta-rl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from cifar10_models.resnet import resnet18\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cc1e8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g0/93h1vwd10rq00w5yx9hftvxm0000gq/T/ipykernel_45565/3764615324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                download=False, transform=transform)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcifar_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# cifar_f_test = torchvision.datasets.CIFAR10(root='./data', train=False,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#                                                download=False, transform=transform)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "cifar_gaussian_noise = np.load(\"CIFAR-10-C/gaussian_noise.npy\")\n",
    "cifar_labels = np.load(\"CIFAR-10-C/labels.npy\")\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    ")\n",
    "\n",
    "cifar_f = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                               download=True, transform=transform)\n",
    "\n",
    "# cifar_f_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                                download=False, transform=transform)\n",
    "cifar_gn = [0]*len(cifar_gaussian_noise)\n",
    "for i in range(len(cifar_gaussian_noise)):\n",
    "    cifar_gn[i] = transform(cifar_gaussian_noise[i])\n",
    "for i in range(len(cifar_f.targets)):\n",
    "    cifar_f.targets[i] = 9 - cifar_f.targets[i]\n",
    "    \n",
    "cifar_gnx_train, cifar_gnx_test, cifar_gny_train, cifar_gny_test = train_test_split(cifar_gn, \n",
    "                                                                                    cifar_labels, test_size=0.30, \n",
    "                                                                                    random_state=42)\n",
    "\n",
    "cifar_fx_train, cifar_fx_test, cifar_fy_train, cifar_fy_test = train_test_split(cifar_f.data, \n",
    "                                                                                    cifar_f.targets, test_size=0.30, \n",
    "                                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b33663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset:\n",
    "    def __init__(self, t0_x, t1_x, t0_y, t1_y):\n",
    "        self.t0_x = t0_x\n",
    "        self.t1_x = t1_x\n",
    "        self.t0_y = t0_y\n",
    "        self.t1_y = t1_y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.t0_x[idx], self.t1_x[idx], self.t0_y[idx], self.t1_y[idx]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.t0_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3857a95",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiTaskDataset' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g0/93h1vwd10rq00w5yx9hftvxm0000gq/T/ipykernel_45565/2744293697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTaskDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_gnx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_fx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_gny_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_fy_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiTaskDataset' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "X_train = MultiTaskDataset(cifar_gnx_train, cifar_fx_train, cifar_gny_train, cifar_fy_train)\n",
    "X_test = MultiTaskDataset(cifar_gnx_test, cifar_fx_test, cifar_gny_test, cifar_fy_test)\n",
    "\n",
    "trainloader = DataLoader(X_train, batch_size=256, shuffle=True)\n",
    "testloader = DataLoader(X_test, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d7c27c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_resnet(nn.Module):\n",
    "    def __init__(self, shared=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet_backbone = resnet18(pretrained=True)\n",
    "        self.resnet_backbone = nn.Sequential(*list(self.resnet_backbone.children())[:-1])\n",
    "        \n",
    "        self.t0_head = nn.Linear(512, 10)\n",
    "        self.t1_head = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, t0_x, t1_x):\n",
    "        t0_resnet = self.resnet_backbone(t0_x)\n",
    "        t1_resnet = self.resnet_backbone(t1_x)\n",
    "        \n",
    "        b, h, _, _ = t0_resnet.shape\n",
    "        t0_logits = self.t0_head(t0_resnet.view((b, h)))\n",
    "        t1_logits = self.t1_head(t1_resnet.view((b, h)))\n",
    "        \n",
    "        return t0_logits, t1_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d29386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy()\n",
    "\n",
    "def train_mtl_rgn(\n",
    "    *, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    lr=1e-3, \n",
    "    epochs=5, \n",
    "):\n",
    "    model = MTL_resnet()\n",
    "    t0_model = MTL_resnet()\n",
    "    t1_model = MTL_resnet()\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    t0_model.to(DEVICE)\n",
    "    t1_model.to(DEVICE)\n",
    "    \n",
    "    linear_lr = []\n",
    "    shared_lr = []\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"backbone\" in k and \"weight\" in k:\n",
    "            linear_lr.append(v)\n",
    "        else:\n",
    "            shared_lr.append(v)\n",
    "#     parameters = [{\"params\": linear_lr}, {\"params\": shared_lr}]\n",
    "    parameters = [{\"params\": linear_lr}, {\"params\": shared_lr}]\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    t0_optimizer = torch.optim.Adam(optimizer.param_groups, lr = lr)\n",
    "    t1_optimizer = torch.optim.Adam(optimizer.param_groups, lr = lr)\n",
    "    \n",
    "    train_metrics = {'loss/total': [], \n",
    "                     'loss/t0': [], \n",
    "                     'loss/t1': [],\n",
    "                     'acc/t0': [],\n",
    "                     'acc/t1': [],\n",
    "                    }\n",
    "    val_metrics = {'loss/total': [], \n",
    "                     'loss/t0': [], \n",
    "                     'loss/t1': [],\n",
    "                     'acc/t0': [],\n",
    "                     'acc/t1': [],\n",
    "                    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        last_it = len(train_loader)\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):    \n",
    "            t0_x, t1_x, t0_labels, t1_labels = batch\n",
    "            t0_x, t1_x, t0_labels, t1_labels = t0_x.to(DEVICE), t1_x.to(DEVICE), t0_labels.to(DEVICE), t1_labels.to(DEVICE)\n",
    "\n",
    "            # t0_x = torch.permute(t0_x, (0, 3, 1, 2)).float()\n",
    "            t1_x = torch.permute(t1_x, (0, 3, 1, 2)).float()\n",
    "\n",
    "            t0_logits, t1_logits = model.forward(t0_x, t1_x)\n",
    "            \n",
    "#             t0_logits = t0_logits.view(-1,)\n",
    "#             t1_logits = t1_logits.view(-1,)\n",
    "            \n",
    "            t0_loss = nn.functional.cross_entropy(t0_logits, t0_labels)\n",
    "            t1_loss = nn.functional.cross_entropy(t1_logits, t1_labels)\n",
    "            \n",
    "            total_loss = (t0_loss + t1_loss)/2\n",
    "            \n",
    "            t0_loss.backward(retain_graph=True)\n",
    "            t0rgn = 1\n",
    "            t1rgn = 1\n",
    "            if i == last_it:\n",
    "                t0_model.load_state_dict(model.state_dict())\n",
    "                t0_optimizer.load_state_dict(optimizer.state_dict())\n",
    "                t1_model.load_state_dict(model.state_dict())\n",
    "                t1_optimizer.load_state_dict(optimizer.state_dict())\n",
    "            if i >= last_it:\n",
    "                for g, h in zip(t0_optimizer.param_groups, t1_optimizer.param_groups):\n",
    "                    t0grad = torch.autograd.grad(t0_loss, g[\"params\"][0], allow_unused=True, retain_graph=True)\n",
    "                    t1grad = torch.autograd.grad(t1_loss, h[\"params\"][0], allow_unused=True, retain_graph=True)\n",
    "                    \n",
    "                    if t0grad[0] is not None:\n",
    "                        t0rgn = torch.linalg.norm(t0grad[0]/torch.linalg.norm(g[\"params\"][0]))\n",
    "                    if t1grad[0] is not None:\n",
    "                        t1rgn = torch.linalg.norm(t1grad[0]/torch.linalg.norm(h[\"params\"][0]))\n",
    "                    t0rgn, t1rgn = nn.functional.softmax(torch.Tensor([t0rgn, t1rgn]))\n",
    "                    g[\"lr\"] = t0rgn*g[\"lr\"]\n",
    "                    h[\"lr\"] = t1rgn*h[\"lr\"]\n",
    "                    \n",
    "                    t0rgn = 1\n",
    "                    t1rgn = 1\n",
    "                    \n",
    "                    \n",
    "                t0_optimizer.step()\n",
    "                t1_optimizer.step()\n",
    "                \n",
    "                t0_optimizer.zero_grad()\n",
    "                t1_optimizer.zero_grad()\n",
    "                \n",
    "                # loss metrics\n",
    "                train_metrics[\"loss/total\"].append(total_loss.item())\n",
    "                train_metrics[\"loss/t0\"].append(t0_loss.item())\n",
    "                train_metrics[\"loss/t1\"].append(t1_loss.item())\n",
    "\n",
    "                # accuracy metrics\n",
    "                t0_lgt, _ = t0_model.forward(t0_x, t1_x)\n",
    "                _, t1_lgt = t1_model.forward(t0_x, t1_x)\n",
    "                t0_lgt, t1_lgt = t0_lgt.clone().detach().cpu(), t1_lgt.clone().detach().cpu()\n",
    "                t0_lbl = t0_labels.clone().detach().cpu()\n",
    "                t1_lbl = t1_labels.clone().detach().cpu()\n",
    "#                 t0_logits, t1_logits = model.forward(im)\n",
    "                train_metrics['acc/t0'].append(accuracy(t0_lgt, t0_lbl))\n",
    "                train_metrics['acc/t1'].append(accuracy(t1_lgt, t1_lbl))\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # loss metrics\n",
    "                train_metrics[\"loss/total\"].append(total_loss.item())\n",
    "                train_metrics[\"loss/t0\"].append(t0_loss.item())\n",
    "                train_metrics[\"loss/t1\"].append(t1_loss.item())\n",
    "\n",
    "                # accuracy metrics\n",
    "                t0_lgt, t1_lgt = model.forward(t0_x, t1_x)\n",
    "                t0_lgt, t1_lgt = t0_lgt.clone().detach().cpu(), t1_lgt.clone().detach().cpu()\n",
    "                t0_lbl = t0_labels.clone().detach().cpu()\n",
    "                t1_lbl = t1_labels.clone().detach().cpu()\n",
    "#                 t0_pred = nn.functional.sigmoid(t0_lgt) > 0.5\n",
    "#                 t1_pred = nn.functional.sigmoid(t1_lgt) > 0.5\n",
    "                train_metrics['acc/t0'].append(accuracy(t0_lgt, t0_lbl))\n",
    "                train_metrics['acc/t1'].append(accuracy(t1_lgt, t1_lbl))\n",
    "\n",
    "\n",
    "        t0_val_loss = 0\n",
    "        t1_val_loss = 0\n",
    "        t0_val_acc = 0\n",
    "        t1_val_acc = 0\n",
    "        val_count = 0\n",
    "\n",
    "        model.eval()\n",
    "        t0_model.eval()\n",
    "        t1_model.eval()\n",
    "        for batch in test_loader:\n",
    "            t0_x, t1_x, t0_labels, t1_labels = batch\n",
    "            t0_x, t1_x, t0_labels, t1_labels = t0_x.to(DEVICE), t1_x.to(DEVICE), t0_labels.to(DEVICE), t1_labels.to(DEVICE)\n",
    "#             val_loss = model.loss(im, t0_labels.float(), t1_labels.float())\n",
    "            # t0_x = torch.permute(t0_x, (0, 3, 1, 2)).float()\n",
    "            t1_x = torch.permute(t1_x, (0, 3, 1, 2)).float()\n",
    "\n",
    "            t0_logits, t1_logits = t0_model.forward(t0_x, t1_x)\n",
    "            \n",
    "#             t0_logits = t0_logits.view(-1,)\n",
    "#             t1_logits = t1_logits.view(-1,)\n",
    "            \n",
    "            t0_loss = nn.functional.cross_entropy(t0_logits, t0_labels)\n",
    "            t1_loss = nn.functional.cross_entropy(t1_logits, t1_labels)\n",
    "            total_loss = (t0_loss + t1_loss)/2\n",
    "            \n",
    "            val_count += len(t0_x)\n",
    "            t0_val_loss += t0_loss.item() * len(t0_x)\n",
    "            t1_val_loss += t1_loss.item() * len(t1_x)\n",
    "\n",
    "            # accuracy metrics\n",
    "#             t0_logits, _ = t0_model.forward(t0_x, t1_x)\n",
    "#             _, t1_logits = t1_model.forward(t0_x, t1_x)\n",
    "            t0_lgt, t1_lgt = model.forward(t0_x, t1_x)\n",
    "            t0_lgt, t1_lgt = t0_lgt.clone().detach().cpu(), t1_lgt.clone().detach().cpu()\n",
    "            t0_lbl = t0_labels.clone().detach().cpu()\n",
    "            t1_lbl = t1_labels.clone().detach().cpu()\n",
    "            t0_val_acc += accuracy(t0_lgt, t0_lbl) * len(t0_x)\n",
    "            t1_val_acc += accuracy(t1_lgt, t1_lbl) * len(t1_x)\n",
    "\n",
    "        val_metrics['loss/t0'].append(t0_val_loss / val_count)\n",
    "        val_metrics['loss/t1'].append(t1_val_loss / val_count)\n",
    "        val_metrics['acc/t0'].append(t0_val_acc / val_count)\n",
    "        val_metrics['acc/t1'].append(t1_val_acc / val_count)\n",
    "    \n",
    "    return train_metrics, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0171527d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g0/93h1vwd10rq00w5yx9hftvxm0000gq/T/ipykernel_45565/304718315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m _, shared_metrics = train_mtl_rgn(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "_, shared_metrics = train_mtl_rgn(\n",
    "    train_loader=trainloader, \n",
    "    test_loader=testloader,\n",
    "    lr=1e-3, \n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba308252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustbench.utils import load_model\n",
    "from robustbench.data import load_cifar10c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15ee1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = ['gaussian_noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973c530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://zenodo.org/api/files/a35f793a-6997-4603-a92f-926a6bc0fa60/CIFAR-10-C.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44533it [23:17, 31.87it/s]                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished, extracting...\n",
      "Downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = load_cifar10c(n_examples=1000, corruptions=corruptions, severity=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name, dataset='cifar10', threat_model='Linf')\n",
    "acc = clean_accuracy(model, x_test, y_test)\n",
    "print(f'Model: {model_name}, CIFAR-10-C accuracy: {acc:.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Meta-RL (ipykernel)",
   "language": "python",
   "name": "meta-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
